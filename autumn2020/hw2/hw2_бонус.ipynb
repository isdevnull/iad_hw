{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.9-final"
    },
    "colab": {
      "name": "hw2-бонус.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K251vAejYJHT"
      },
      "source": [
        "# Домашнее задание 2. Бонус"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MY-sYLUGYJHU"
      },
      "source": [
        "Оценка за часть 1 и часть 2 в этом дз -- по 5 баллов."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gcgXlIWzm2Hs",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "50ea8554-79ea-4bbc-a818-48088eb210b0"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nw-nCWUxnDax"
      },
      "source": [
        "!unzip -qq /content/drive/My\\ Drive/dataset.zip "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZCtFfgCyYTyE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cad5d426-acd5-48bf-f1fb-9637396e782a"
      },
      "source": [
        "! pip install albumentations==0.4.6"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting albumentations==0.4.6\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/92/33/1c459c2c9a4028ec75527eff88bc4e2d256555189f42af4baf4d7bd89233/albumentations-0.4.6.tar.gz (117kB)\n",
            "\u001b[K     |████████████████████████████████| 122kB 9.9MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.11.1 in /usr/local/lib/python3.6/dist-packages (from albumentations==0.4.6) (1.18.5)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from albumentations==0.4.6) (1.4.1)\n",
            "Collecting imgaug>=0.4.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/66/b1/af3142c4a85cba6da9f4ebb5ff4e21e2616309552caca5e8acefe9840622/imgaug-0.4.0-py2.py3-none-any.whl (948kB)\n",
            "\u001b[K     |████████████████████████████████| 952kB 19.6MB/s \n",
            "\u001b[?25hRequirement already satisfied: PyYAML in /usr/local/lib/python3.6/dist-packages (from albumentations==0.4.6) (3.13)\n",
            "Requirement already satisfied: opencv-python>=4.1.1 in /usr/local/lib/python3.6/dist-packages (from albumentations==0.4.6) (4.1.2.30)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.6/dist-packages (from imgaug>=0.4.0->albumentations==0.4.6) (3.2.2)\n",
            "Requirement already satisfied: Shapely in /usr/local/lib/python3.6/dist-packages (from imgaug>=0.4.0->albumentations==0.4.6) (1.7.1)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.6/dist-packages (from imgaug>=0.4.0->albumentations==0.4.6) (7.0.0)\n",
            "Requirement already satisfied: imageio in /usr/local/lib/python3.6/dist-packages (from imgaug>=0.4.0->albumentations==0.4.6) (2.4.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from imgaug>=0.4.0->albumentations==0.4.6) (1.15.0)\n",
            "Requirement already satisfied: scikit-image>=0.14.2 in /usr/local/lib/python3.6/dist-packages (from imgaug>=0.4.0->albumentations==0.4.6) (0.16.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.6/dist-packages (from matplotlib->imgaug>=0.4.0->albumentations==0.4.6) (0.10.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->imgaug>=0.4.0->albumentations==0.4.6) (1.3.1)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->imgaug>=0.4.0->albumentations==0.4.6) (2.4.7)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->imgaug>=0.4.0->albumentations==0.4.6) (2.8.1)\n",
            "Requirement already satisfied: PyWavelets>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from scikit-image>=0.14.2->imgaug>=0.4.0->albumentations==0.4.6) (1.1.1)\n",
            "Requirement already satisfied: networkx>=2.0 in /usr/local/lib/python3.6/dist-packages (from scikit-image>=0.14.2->imgaug>=0.4.0->albumentations==0.4.6) (2.5)\n",
            "Requirement already satisfied: decorator>=4.3.0 in /usr/local/lib/python3.6/dist-packages (from networkx>=2.0->scikit-image>=0.14.2->imgaug>=0.4.0->albumentations==0.4.6) (4.4.2)\n",
            "Building wheels for collected packages: albumentations\n",
            "  Building wheel for albumentations (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for albumentations: filename=albumentations-0.4.6-cp36-none-any.whl size=65165 sha256=cece7a6958e4d185884edb7111353594b64b872dfee5bce246afcc98914f6cf2\n",
            "  Stored in directory: /root/.cache/pip/wheels/c7/f4/89/56d1bee5c421c36c1a951eeb4adcc32fbb82f5344c086efa14\n",
            "Successfully built albumentations\n",
            "Installing collected packages: imgaug, albumentations\n",
            "  Found existing installation: imgaug 0.2.9\n",
            "    Uninstalling imgaug-0.2.9:\n",
            "      Successfully uninstalled imgaug-0.2.9\n",
            "  Found existing installation: albumentations 0.1.12\n",
            "    Uninstalling albumentations-0.1.12:\n",
            "      Successfully uninstalled albumentations-0.1.12\n",
            "Successfully installed albumentations-0.4.6 imgaug-0.4.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5zG4s6V6YJHW"
      },
      "source": [
        "#IMPORTS\n",
        "import sys\n",
        "import torch\n",
        "import torchvision\n",
        "from torch import nn\n",
        "from torch.nn import functional as F\n",
        "import numpy as np\n",
        "import random\n",
        "\n",
        "import cv2\n",
        "import albumentations as A\n",
        "from albumentations.pytorch import ToTensorV2, ToTensor\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4EFEjSKoYJHc"
      },
      "source": [
        "train_transform_no_resize = A.Compose(\n",
        "    [\n",
        "        A.PadIfNeeded(min_height=32, min_width=32, value=4),\n",
        "        A.RandomCrop(height=28, width=28),\n",
        "        A.Flip(),\n",
        "        A.Normalize(),\n",
        "        ToTensorV2()\n",
        "    ]\n",
        ")\n",
        "\n",
        "val_transform_no_resize = A.Compose(\n",
        "    [\n",
        "        A.Normalize(),\n",
        "        ToTensorV2()\n",
        "    ]\n",
        ")\n",
        "\n",
        "train_no_resize_func = lambda img: train_transform_no_resize(image=np.array(img))\n",
        "val_no_resize_func = lambda img: val_transform_no_resize(image=np.array(img))\n",
        "# YOU CAN DEFINE AUGMENTATIONS HERE\n",
        "\n",
        "train_dataset = torchvision.datasets.ImageFolder(\"./dataset/dataset/train\", transform=train_no_resize_func)\n",
        "val_dataset = torchvision.datasets.ImageFolder(\"./dataset/dataset/val\", transform=val_no_resize_func)\n",
        "# REPLACE ./dataset/dataset WITH THE FOLDER WHERE YOU DOWNLOADED AND UNZIPPED THE DATASET\n",
        "# OR USE torchvision.datasets.ImageFolder INSTEAD OF MyDataset"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zKs6vDZ01wFe"
      },
      "source": [
        "Посмотрим как выглядят картинки с предложенной аугментацией:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gkV4ZnUBYJHg"
      },
      "source": [
        "sample_transform = A.Compose(\n",
        "    [\n",
        "        A.PadIfNeeded(min_height=32, min_width=32, value=4),\n",
        "        A.RandomCrop(height=32, width=32),\n",
        "        A.Flip(),\n",
        "        A.CoarseDropout(max_holes=1, max_height=10, max_width=10),\n",
        "        #A.Normalize(),\n",
        "    ]\n",
        ")\n",
        "\n",
        "def visualize_before_after():\n",
        "    fig, ax = plt.subplots(nrows=1, ncols=2, figsize=(10,10))\n",
        "    image = cv2.imread(\"./dataset/dataset/train/class_191/00002.jpg\")\n",
        "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "    ax[0].imshow(image)\n",
        "    image = sample_transform(image=image)['image']\n",
        "    ax[1].imshow(image)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ol8FtObNYJHj",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 304
        },
        "outputId": "19d7f00c-b45a-4af8-f3d5-c87f25386a38"
      },
      "source": [
        "# SAMPLE VISUALIZATION\n",
        "visualize_before_after()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 720x720 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lf1zYh6xYJHm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "203bf798-4d24-4875-a787-7e824210853b"
      },
      "source": [
        "# Just very simple checks\n",
        "assert isinstance(train_dataset[0], tuple)\n",
        "assert len(train_dataset[0]) == 2\n",
        "assert isinstance(train_dataset[1][1], int)\n",
        "print(\"tests passed\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tests passed\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sHfMwGuNYJHo"
      },
      "source": [
        "from tqdm import tqdm\n",
        "from sklearn.metrics import accuracy_score"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J0p4BIrVYJHr"
      },
      "source": [
        "dataloader_handler = {\n",
        "    \"train\": torch.utils.data.DataLoader(train_dataset, batch_size=128, shuffle=True, num_workers=4),\n",
        "    \"val\": torch.utils.data.DataLoader(val_dataset, batch_size=512, shuffle=False, num_workers=4)\n",
        "}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6XZW2AnyYJHv"
      },
      "source": [
        "train_loss = []\n",
        "train_acc = []\n",
        "def train_one_epoch(model, train_dataloader, criterion, optimizer, device=\"cuda:0\"):\n",
        "    model.train()\n",
        "    with tqdm(total=len(train_dataloader), file=sys.stdout) as prbar:\n",
        "      for inputs, labels in train_dataloader:\n",
        "        inputs = inputs['image'].to(device)\n",
        "        labels = labels.to(device)\n",
        "        outputs = model(inputs) # feed forward\n",
        "        optimizer.zero_grad()\n",
        "        acc = (outputs.argmax(1) == labels).float().mean() # accuracy(outputs, labels), compute accuracy\n",
        "        loss = criterion(outputs, labels) # compute loss\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        prbar.set_description(\n",
        "            \"batch tloss: %.4f, batch tacc: %.4f\" % (loss.item(), acc.item())\n",
        "        ) \n",
        "        prbar.update(1)\n",
        "        train_loss.append(loss.cpu().detach().item())\n",
        "        train_acc.append(acc.cpu().detach().item())\n",
        "\n",
        "@torch.no_grad()\n",
        "def predict(model, val_dataloder, criterion, device=\"cuda:0\"):\n",
        "    model.to(device) # prevent errors when using single function outside training loop\n",
        "    losses = []\n",
        "    true_classes = np.array([])\n",
        "    predicted_classes = np.array([])\n",
        "    model.eval()\n",
        "    with tqdm(total=len(val_dataloader), file=sys.stdout) as prbar:\n",
        "        for inputs, labels in val_dataloader:\n",
        "            inputs = inputs['image'].to(device)\n",
        "            labels = labels.to(device)\n",
        "            outputs = model(inputs) # feed forward\n",
        "            loss = criterion(outputs, labels)\n",
        "            acc = (outputs.argmax(1) == labels).float().mean()\n",
        "            losses.append(loss.item())\n",
        "            prbar.set_description(\n",
        "                \"batch vloss: %.4f, batch vacc: %.4f\" % (loss.item(), acc.item())\n",
        "            ) \n",
        "            prbar.update(1)\n",
        "            true_classes = np.append(true_classes, labels.cpu().detach().numpy())\n",
        "            predicted_classes = np.append(predicted_classes, outputs.argmax(1).cpu().detach().numpy())\n",
        "    return np.array(losses), predicted_classes, true_classes\n",
        "\n",
        "\n",
        "def train(model, train_dataloader, val_dataloader, criterion, optimizer, device=\"cuda:0\", n_epochs=10, scheduler=None):\n",
        "    model.to(device)\n",
        "    e_loss = []\n",
        "    e_acc = []\n",
        "    for epoch in range(n_epochs):\n",
        "        print(\"Epoch[%d] training...\" % epoch)\n",
        "        train_one_epoch(model, train_dataloader, criterion, optimizer, device)\n",
        "        losses, predicted_classes, true_classes = predict(model, val_dataloader, criterion, device)\n",
        "        mean_loss = losses.mean()\n",
        "        metrics = (mean_loss, (predicted_classes == true_classes).mean())\n",
        "        if scheduler:\n",
        "            scheduler.step()\n",
        "        print(\"epoch loss: %.4f, epoch acc: %.4f\" % (metrics[0], metrics[1]))\n",
        "        # Train, evaluate, print accuracy, make a step of scheduler or whatever you want...\n",
        "        e_loss.append(metrics[0])\n",
        "        e_acc.append(metrics[1])\n",
        "    return e_loss, e_acc\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zvIQkxA4YJHx"
      },
      "source": [
        "import torchvision.models as models"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yrtJTsZFVs70",
        "outputId": "d722a9bb-b1b3-4f7d-eaa5-40dfdaedc4f5"
      },
      "source": [
        "torch.manual_seed(42)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<torch._C.Generator at 0x7fb4bc8bbbd0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M-1ANs3XK9yr"
      },
      "source": [
        "class ModifiedResNet18(torch.nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.resnet = models.resnet18(pretrained=False)\n",
        "        self.resnet.conv1 = torch.nn.Conv2d(\n",
        "            3, 64, kernel_size=3, stride=1, padding=1, bias=False # use it instead of maxpool and change kernel size to lower 3x3 instead of 7x7\n",
        "        )\n",
        "        self.resnet.maxpool = torch.nn.Identity()\n",
        "        self.resnet.fc = nn.Linear(512, 200)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.resnet(x)\n",
        "        return x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zn1snrjMhIu_"
      },
      "source": [
        "resnet18 = ModifiedResNet18()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "EeSJJHgkYJH4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2dcee7cd-5667-461a-a1e8-1824481012fc"
      },
      "source": [
        "model = resnet18 # THE MODEL THAT YOU CHOOSE\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=0.05, momentum=0.9, weight_decay=5e-4) # YOUR OPTIMIZER\n",
        "train_dataloader = dataloader_handler['train'] # TRAIN DATALOADER WHICH YOU CONSTRUCT\n",
        "val_dataloader = dataloader_handler['val'] # VAL DATALOADER WHICH YOU CONSTRUCT\n",
        "criterion = nn.CrossEntropyLoss() # nn.CrossEntropyLoss() # LOSS THAT YOU OPTIMIZE (SHOLD BE CROSS ENTROPY OR SMTH ELSE)\n",
        "scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer, milestones=[20, 40], gamma=0.1) # LR SCHEDULE THAT YOU PROBABLY CHOOSE\n",
        "n_epochs = 50 # NUMBER OF EPOCHS\n",
        "device = torch.device(\"cuda:0\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
        "print(device)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "cuda:0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iIt1DP7nYJH6"
      },
      "source": [
        "Простой тест на проверку правильности написанного кода"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SPcxYR_TYJH7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6a15d5be-afa8-4170-f544-42198c126b67"
      },
      "source": [
        "all_losses, predicted_labels, true_labels = predict(model, val_dataloader, criterion, device)\n",
        "print(len(predicted_labels), len(val_dataset))\n",
        "assert len(predicted_labels) == len(val_dataset)\n",
        "accuracy = accuracy_score(predicted_labels, true_labels)\n",
        "print(accuracy)\n",
        "print(\"tests passed\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "batch vloss: 7.3624, batch vacc: 0.0000: 100%|██████████| 20/20 [00:07<00:00,  2.82it/s]\n",
            "10000 10000\n",
            "0.005\n",
            "tests passed\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2GfNDco4YJH9"
      },
      "source": [
        "Запустить обучение можно в ячейке ниже."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zSU3MYWlYJH9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a9412067-da83-4aa1-e579-a697260dbd0c"
      },
      "source": [
        "train(resnet18, train_dataloader, val_dataloader, criterion, optimizer, device, n_epochs, scheduler)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch[0] training...\n",
            "batch tloss: 4.5904, batch tacc: 0.0625: 100%|██████████| 782/782 [01:03<00:00, 12.22it/s]\n",
            "batch vloss: 4.7218, batch vacc: 0.0551: 100%|██████████| 20/20 [00:07<00:00,  2.81it/s]\n",
            "epoch loss: 4.6201, epoch acc: 0.0515\n",
            "Epoch[1] training...\n",
            "batch tloss: 4.5356, batch tacc: 0.0938: 100%|██████████| 782/782 [01:03<00:00, 12.39it/s]\n",
            "batch vloss: 4.0304, batch vacc: 0.1250: 100%|██████████| 20/20 [00:07<00:00,  2.76it/s]\n",
            "epoch loss: 4.1185, epoch acc: 0.1271\n",
            "Epoch[2] training...\n",
            "batch tloss: 4.2222, batch tacc: 0.0625: 100%|██████████| 782/782 [01:03<00:00, 12.34it/s]\n",
            "batch vloss: 3.6259, batch vacc: 0.1618: 100%|██████████| 20/20 [00:07<00:00,  2.83it/s]\n",
            "epoch loss: 4.1132, epoch acc: 0.1197\n",
            "Epoch[3] training...\n",
            "batch tloss: 3.4907, batch tacc: 0.1562: 100%|██████████| 782/782 [01:03<00:00, 12.36it/s]\n",
            "batch vloss: 3.3528, batch vacc: 0.3162: 100%|██████████| 20/20 [00:07<00:00,  2.81it/s]\n",
            "epoch loss: 3.8028, epoch acc: 0.1628\n",
            "Epoch[4] training...\n",
            "batch tloss: 3.7778, batch tacc: 0.1875: 100%|██████████| 782/782 [01:03<00:00, 12.34it/s]\n",
            "batch vloss: 3.2909, batch vacc: 0.3125: 100%|██████████| 20/20 [00:06<00:00,  2.86it/s]\n",
            "epoch loss: 3.7269, epoch acc: 0.1827\n",
            "Epoch[5] training...\n",
            "batch tloss: 4.1643, batch tacc: 0.0938: 100%|██████████| 782/782 [01:03<00:00, 12.36it/s]\n",
            "batch vloss: 3.4360, batch vacc: 0.3235: 100%|██████████| 20/20 [00:07<00:00,  2.82it/s]\n",
            "epoch loss: 3.6221, epoch acc: 0.2058\n",
            "Epoch[6] training...\n",
            "batch tloss: 3.8374, batch tacc: 0.2188: 100%|██████████| 782/782 [01:02<00:00, 12.46it/s]\n",
            "batch vloss: 3.2864, batch vacc: 0.2537: 100%|██████████| 20/20 [00:07<00:00,  2.82it/s]\n",
            "epoch loss: 3.7058, epoch acc: 0.1985\n",
            "Epoch[7] training...\n",
            "batch tloss: 3.5681, batch tacc: 0.2188: 100%|██████████| 782/782 [01:02<00:00, 12.59it/s]\n",
            "batch vloss: 3.5155, batch vacc: 0.1949: 100%|██████████| 20/20 [00:06<00:00,  2.86it/s]\n",
            "epoch loss: 3.5791, epoch acc: 0.2212\n",
            "Epoch[8] training...\n",
            "batch tloss: 3.4518, batch tacc: 0.3125: 100%|██████████| 782/782 [01:02<00:00, 12.55it/s]\n",
            "batch vloss: 3.0491, batch vacc: 0.3824: 100%|██████████| 20/20 [00:07<00:00,  2.81it/s]\n",
            "epoch loss: 3.5089, epoch acc: 0.2396\n",
            "Epoch[9] training...\n",
            "batch tloss: 3.9105, batch tacc: 0.0625: 100%|██████████| 782/782 [01:03<00:00, 12.26it/s]\n",
            "batch vloss: 3.2884, batch vacc: 0.2794: 100%|██████████| 20/20 [00:07<00:00,  2.82it/s]\n",
            "epoch loss: 3.4668, epoch acc: 0.2478\n",
            "Epoch[10] training...\n",
            "batch tloss: 3.4214, batch tacc: 0.1562: 100%|██████████| 782/782 [01:02<00:00, 12.41it/s]\n",
            "batch vloss: 3.1239, batch vacc: 0.2868: 100%|██████████| 20/20 [00:07<00:00,  2.81it/s]\n",
            "epoch loss: 3.3670, epoch acc: 0.2630\n",
            "Epoch[11] training...\n",
            "batch tloss: 3.1300, batch tacc: 0.4062: 100%|██████████| 782/782 [01:02<00:00, 12.43it/s]\n",
            "batch vloss: 3.2094, batch vacc: 0.3272: 100%|██████████| 20/20 [00:07<00:00,  2.83it/s]\n",
            "epoch loss: 3.2922, epoch acc: 0.2822\n",
            "Epoch[12] training...\n",
            "batch tloss: 3.4852, batch tacc: 0.2500: 100%|██████████| 782/782 [01:02<00:00, 12.49it/s]\n",
            "batch vloss: 3.1296, batch vacc: 0.2978: 100%|██████████| 20/20 [00:07<00:00,  2.82it/s]\n",
            "epoch loss: 3.2669, epoch acc: 0.2856\n",
            "Epoch[13] training...\n",
            "batch tloss: 4.1418, batch tacc: 0.0312: 100%|██████████| 782/782 [01:02<00:00, 12.44it/s]\n",
            "batch vloss: 2.9780, batch vacc: 0.3382: 100%|██████████| 20/20 [00:07<00:00,  2.85it/s]\n",
            "epoch loss: 3.2656, epoch acc: 0.2817\n",
            "Epoch[14] training...\n",
            "batch tloss: 3.7485, batch tacc: 0.1562: 100%|██████████| 782/782 [01:02<00:00, 12.52it/s]\n",
            "batch vloss: 2.9508, batch vacc: 0.3750: 100%|██████████| 20/20 [00:07<00:00,  2.83it/s]\n",
            "epoch loss: 3.2877, epoch acc: 0.2839\n",
            "Epoch[15] training...\n",
            "batch tloss: 3.5596, batch tacc: 0.1875: 100%|██████████| 782/782 [01:02<00:00, 12.46it/s]\n",
            "batch vloss: 2.6330, batch vacc: 0.4265: 100%|██████████| 20/20 [00:07<00:00,  2.81it/s]\n",
            "epoch loss: 3.2516, epoch acc: 0.2888\n",
            "Epoch[16] training...\n",
            "batch tloss: 3.3727, batch tacc: 0.3438: 100%|██████████| 782/782 [01:03<00:00, 12.41it/s]\n",
            "batch vloss: 2.9220, batch vacc: 0.3824: 100%|██████████| 20/20 [00:07<00:00,  2.85it/s]\n",
            "epoch loss: 3.1703, epoch acc: 0.3173\n",
            "Epoch[17] training...\n",
            "batch tloss: 3.7805, batch tacc: 0.1562: 100%|██████████| 782/782 [01:03<00:00, 12.35it/s]\n",
            "batch vloss: 2.8930, batch vacc: 0.3750: 100%|██████████| 20/20 [00:07<00:00,  2.81it/s]\n",
            "epoch loss: 3.1584, epoch acc: 0.3048\n",
            "Epoch[18] training...\n",
            "batch tloss: 4.2786, batch tacc: 0.0938: 100%|██████████| 782/782 [01:03<00:00, 12.35it/s]\n",
            "batch vloss: 3.4773, batch vacc: 0.2647: 100%|██████████| 20/20 [00:07<00:00,  2.79it/s]\n",
            "epoch loss: 3.3324, epoch acc: 0.2672\n",
            "Epoch[19] training...\n",
            "batch tloss: 3.1399, batch tacc: 0.3125: 100%|██████████| 782/782 [01:03<00:00, 12.30it/s]\n",
            "batch vloss: 2.8632, batch vacc: 0.4081: 100%|██████████| 20/20 [00:07<00:00,  2.78it/s]\n",
            "epoch loss: 3.2541, epoch acc: 0.2790\n",
            "Epoch[20] training...\n",
            "batch tloss: 3.1902, batch tacc: 0.2812: 100%|██████████| 782/782 [01:03<00:00, 12.30it/s]\n",
            "batch vloss: 2.5543, batch vacc: 0.4522: 100%|██████████| 20/20 [00:07<00:00,  2.81it/s]\n",
            "epoch loss: 2.6743, epoch acc: 0.4293\n",
            "Epoch[21] training...\n",
            "batch tloss: 2.3906, batch tacc: 0.3125: 100%|██████████| 782/782 [01:03<00:00, 12.27it/s]\n",
            "batch vloss: 2.4345, batch vacc: 0.4963: 100%|██████████| 20/20 [00:07<00:00,  2.79it/s]\n",
            "epoch loss: 2.5938, epoch acc: 0.4460\n",
            "Epoch[22] training...\n",
            "batch tloss: 2.6699, batch tacc: 0.3438: 100%|██████████| 782/782 [01:03<00:00, 12.30it/s]\n",
            "batch vloss: 2.4690, batch vacc: 0.4449: 100%|██████████| 20/20 [00:07<00:00,  2.81it/s]\n",
            "epoch loss: 2.5606, epoch acc: 0.4493\n",
            "Epoch[23] training...\n",
            "batch tloss: 3.0539, batch tacc: 0.2188: 100%|██████████| 782/782 [01:03<00:00, 12.23it/s]\n",
            "batch vloss: 2.3535, batch vacc: 0.5257: 100%|██████████| 20/20 [00:07<00:00,  2.84it/s]\n",
            "epoch loss: 2.5034, epoch acc: 0.4638\n",
            "Epoch[24] training...\n",
            "batch tloss: 2.5323, batch tacc: 0.4375: 100%|██████████| 782/782 [01:03<00:00, 12.28it/s]\n",
            "batch vloss: 2.4458, batch vacc: 0.4596: 100%|██████████| 20/20 [00:07<00:00,  2.83it/s]\n",
            "epoch loss: 2.4837, epoch acc: 0.4625\n",
            "Epoch[25] training...\n",
            "batch tloss: 2.7298, batch tacc: 0.3750: 100%|██████████| 782/782 [01:03<00:00, 12.23it/s]\n",
            "batch vloss: 2.3263, batch vacc: 0.5037: 100%|██████████| 20/20 [00:07<00:00,  2.72it/s]\n",
            "epoch loss: 2.4709, epoch acc: 0.4673\n",
            "Epoch[26] training...\n",
            "batch tloss: 2.8224, batch tacc: 0.3125: 100%|██████████| 782/782 [01:03<00:00, 12.28it/s]\n",
            "batch vloss: 2.3290, batch vacc: 0.4779: 100%|██████████| 20/20 [00:07<00:00,  2.85it/s]\n",
            "epoch loss: 2.4530, epoch acc: 0.4644\n",
            "Epoch[27] training...\n",
            "batch tloss: 2.0098, batch tacc: 0.5938: 100%|██████████| 782/782 [01:03<00:00, 12.27it/s]\n",
            "batch vloss: 2.2910, batch vacc: 0.5184: 100%|██████████| 20/20 [00:07<00:00,  2.81it/s]\n",
            "epoch loss: 2.4616, epoch acc: 0.4696\n",
            "Epoch[28] training...\n",
            "batch tloss: 3.2683, batch tacc: 0.1875: 100%|██████████| 782/782 [01:03<00:00, 12.24it/s]\n",
            "batch vloss: 2.3236, batch vacc: 0.5037: 100%|██████████| 20/20 [00:07<00:00,  2.82it/s]\n",
            "epoch loss: 2.4049, epoch acc: 0.4738\n",
            "Epoch[29] training...\n",
            "batch tloss: 3.2121, batch tacc: 0.1562: 100%|██████████| 782/782 [01:03<00:00, 12.31it/s]\n",
            "batch vloss: 2.2361, batch vacc: 0.5184: 100%|██████████| 20/20 [00:07<00:00,  2.80it/s]\n",
            "epoch loss: 2.4271, epoch acc: 0.4687\n",
            "Epoch[30] training...\n",
            "batch tloss: 3.1742, batch tacc: 0.2812: 100%|██████████| 782/782 [01:03<00:00, 12.27it/s]\n",
            "batch vloss: 2.2862, batch vacc: 0.4926: 100%|██████████| 20/20 [00:07<00:00,  2.77it/s]\n",
            "epoch loss: 2.3691, epoch acc: 0.4826\n",
            "Epoch[31] training...\n",
            "batch tloss: 2.6596, batch tacc: 0.3750: 100%|██████████| 782/782 [01:03<00:00, 12.34it/s]\n",
            "batch vloss: 2.2192, batch vacc: 0.5221: 100%|██████████| 20/20 [00:07<00:00,  2.82it/s]\n",
            "epoch loss: 2.3982, epoch acc: 0.4779\n",
            "Epoch[32] training...\n",
            "batch tloss: 2.8473, batch tacc: 0.3438: 100%|██████████| 782/782 [01:17<00:00, 10.06it/s]\n",
            "batch vloss: 2.3340, batch vacc: 0.4596: 100%|██████████| 20/20 [00:07<00:00,  2.83it/s]\n",
            "epoch loss: 2.3821, epoch acc: 0.4772\n",
            "Epoch[33] training...\n",
            "batch tloss: 2.8533, batch tacc: 0.2812: 100%|██████████| 782/782 [01:05<00:00, 11.87it/s]\n",
            "batch vloss: 2.2055, batch vacc: 0.5221: 100%|██████████| 20/20 [00:07<00:00,  2.77it/s]\n",
            "epoch loss: 2.3575, epoch acc: 0.4823\n",
            "Epoch[34] training...\n",
            "batch tloss: 2.7120, batch tacc: 0.4688: 100%|██████████| 782/782 [01:05<00:00, 11.90it/s]\n",
            "batch vloss: 2.1782, batch vacc: 0.5551: 100%|██████████| 20/20 [00:07<00:00,  2.74it/s]\n",
            "epoch loss: 2.3349, epoch acc: 0.4979\n",
            "Epoch[35] training...\n",
            "batch tloss: 3.0646, batch tacc: 0.3438: 100%|██████████| 782/782 [01:05<00:00, 11.94it/s]\n",
            "batch vloss: 2.0233, batch vacc: 0.5772: 100%|██████████| 20/20 [00:07<00:00,  2.81it/s]\n",
            "epoch loss: 2.3473, epoch acc: 0.4881\n",
            "Epoch[36] training...\n",
            "batch tloss: 2.9116, batch tacc: 0.3750: 100%|██████████| 782/782 [01:05<00:00, 11.93it/s]\n",
            "batch vloss: 2.1510, batch vacc: 0.5184: 100%|██████████| 20/20 [00:07<00:00,  2.78it/s]\n",
            "epoch loss: 2.2977, epoch acc: 0.4973\n",
            "Epoch[37] training...\n",
            "batch tloss: 2.5067, batch tacc: 0.4062: 100%|██████████| 782/782 [01:05<00:00, 11.88it/s]\n",
            "batch vloss: 2.1929, batch vacc: 0.5368: 100%|██████████| 20/20 [00:07<00:00,  2.80it/s]\n",
            "epoch loss: 2.3228, epoch acc: 0.4990\n",
            "Epoch[38] training...\n",
            "batch tloss: 3.0434, batch tacc: 0.3125: 100%|██████████| 782/782 [01:05<00:00, 11.92it/s]\n",
            "batch vloss: 2.2990, batch vacc: 0.5037: 100%|██████████| 20/20 [00:07<00:00,  2.79it/s]\n",
            "epoch loss: 2.3144, epoch acc: 0.4942\n",
            "Epoch[39] training...\n",
            "batch tloss: 2.5635, batch tacc: 0.3438: 100%|██████████| 782/782 [01:05<00:00, 11.98it/s]\n",
            "batch vloss: 2.3617, batch vacc: 0.4596: 100%|██████████| 20/20 [00:07<00:00,  2.72it/s]\n",
            "epoch loss: 2.3244, epoch acc: 0.4917\n",
            "Epoch[40] training...\n",
            "batch tloss: 1.9775, batch tacc: 0.4688: 100%|██████████| 782/782 [01:05<00:00, 11.96it/s]\n",
            "batch vloss: 2.1697, batch vacc: 0.5110: 100%|██████████| 20/20 [00:07<00:00,  2.76it/s]\n",
            "epoch loss: 2.1943, epoch acc: 0.5290\n",
            "Epoch[41] training...\n",
            "batch tloss: 2.6480, batch tacc: 0.4062: 100%|██████████| 782/782 [01:05<00:00, 12.00it/s]\n",
            "batch vloss: 2.0985, batch vacc: 0.5331: 100%|██████████| 20/20 [00:07<00:00,  2.79it/s]\n",
            "epoch loss: 2.1670, epoch acc: 0.5289\n",
            "Epoch[42] training...\n",
            "batch tloss: 2.6958, batch tacc: 0.3750: 100%|██████████| 782/782 [01:04<00:00, 12.05it/s]\n",
            "batch vloss: 2.1137, batch vacc: 0.5257: 100%|██████████| 20/20 [00:07<00:00,  2.80it/s]\n",
            "epoch loss: 2.1542, epoch acc: 0.5335\n",
            "Epoch[43] training...\n",
            "batch tloss: 2.1784, batch tacc: 0.4375: 100%|██████████| 782/782 [01:05<00:00, 12.00it/s]\n",
            "batch vloss: 2.0493, batch vacc: 0.5588: 100%|██████████| 20/20 [00:06<00:00,  2.87it/s]\n",
            "epoch loss: 2.1430, epoch acc: 0.5353\n",
            "Epoch[44] training...\n",
            "batch tloss: 2.5432, batch tacc: 0.3438: 100%|██████████| 782/782 [01:05<00:00, 11.99it/s]\n",
            "batch vloss: 2.0769, batch vacc: 0.5625: 100%|██████████| 20/20 [00:07<00:00,  2.82it/s]\n",
            "epoch loss: 2.1386, epoch acc: 0.5380\n",
            "Epoch[45] training...\n",
            "batch tloss: 2.9600, batch tacc: 0.4062: 100%|██████████| 782/782 [01:04<00:00, 12.04it/s]\n",
            "batch vloss: 2.0501, batch vacc: 0.5588: 100%|██████████| 20/20 [00:07<00:00,  2.81it/s]\n",
            "epoch loss: 2.1316, epoch acc: 0.5349\n",
            "Epoch[46] training...\n",
            "batch tloss: 2.9790, batch tacc: 0.3750: 100%|██████████| 782/782 [01:04<00:00, 12.07it/s]\n",
            "batch vloss: 2.0543, batch vacc: 0.5588: 100%|██████████| 20/20 [00:07<00:00,  2.80it/s]\n",
            "epoch loss: 2.1281, epoch acc: 0.5354\n",
            "Epoch[47] training...\n",
            "batch tloss: 2.7660, batch tacc: 0.2812: 100%|██████████| 782/782 [01:05<00:00, 12.01it/s]\n",
            "batch vloss: 2.0386, batch vacc: 0.5625: 100%|██████████| 20/20 [00:07<00:00,  2.85it/s]\n",
            "epoch loss: 2.1152, epoch acc: 0.5400\n",
            "Epoch[48] training...\n",
            "batch tloss: 2.4311, batch tacc: 0.4062: 100%|██████████| 782/782 [01:04<00:00, 12.08it/s]\n",
            "batch vloss: 2.0374, batch vacc: 0.5588: 100%|██████████| 20/20 [00:07<00:00,  2.70it/s]\n",
            "epoch loss: 2.1198, epoch acc: 0.5362\n",
            "Epoch[49] training...\n",
            "batch tloss: 2.6383, batch tacc: 0.3750: 100%|██████████| 782/782 [01:05<00:00, 11.96it/s]\n",
            "batch vloss: 2.0426, batch vacc: 0.5515: 100%|██████████| 20/20 [00:07<00:00,  2.74it/s]\n",
            "epoch loss: 2.1087, epoch acc: 0.5419\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "([4.620053815841675,\n",
              "  4.118547737598419,\n",
              "  4.1132360935211185,\n",
              "  3.802778720855713,\n",
              "  3.7269425868988035,\n",
              "  3.6220829486846924,\n",
              "  3.705846679210663,\n",
              "  3.5791345953941347,\n",
              "  3.5088521003723145,\n",
              "  3.4667664408683776,\n",
              "  3.3669970512390135,\n",
              "  3.2922337532043455,\n",
              "  3.266925942897797,\n",
              "  3.265609693527222,\n",
              "  3.2877191424369814,\n",
              "  3.2515648722648622,\n",
              "  3.170326018333435,\n",
              "  3.1584096550941467,\n",
              "  3.3323511242866517,\n",
              "  3.254070246219635,\n",
              "  2.6742778062820434,\n",
              "  2.5938212633132935,\n",
              "  2.560631000995636,\n",
              "  2.503417432308197,\n",
              "  2.4836806058883667,\n",
              "  2.4709431409835814,\n",
              "  2.4529536247253416,\n",
              "  2.461648499965668,\n",
              "  2.4048771500587462,\n",
              "  2.427120441198349,\n",
              "  2.3690937399864196,\n",
              "  2.3981608152389526,\n",
              "  2.3820932865142823,\n",
              "  2.3575494647026063,\n",
              "  2.3348562955856322,\n",
              "  2.3472585439682008,\n",
              "  2.2976770043373107,\n",
              "  2.3228366494178774,\n",
              "  2.3144198060035706,\n",
              "  2.3244169116020204,\n",
              "  2.194264602661133,\n",
              "  2.1670078456401827,\n",
              "  2.1542411923408507,\n",
              "  2.1430228412151338,\n",
              "  2.1386182367801667,\n",
              "  2.131634521484375,\n",
              "  2.1280937492847443,\n",
              "  2.115183651447296,\n",
              "  2.119832068681717,\n",
              "  2.1087038457393645],\n",
              " [0.0515,\n",
              "  0.1271,\n",
              "  0.1197,\n",
              "  0.1628,\n",
              "  0.1827,\n",
              "  0.2058,\n",
              "  0.1985,\n",
              "  0.2212,\n",
              "  0.2396,\n",
              "  0.2478,\n",
              "  0.263,\n",
              "  0.2822,\n",
              "  0.2856,\n",
              "  0.2817,\n",
              "  0.2839,\n",
              "  0.2888,\n",
              "  0.3173,\n",
              "  0.3048,\n",
              "  0.2672,\n",
              "  0.279,\n",
              "  0.4293,\n",
              "  0.446,\n",
              "  0.4493,\n",
              "  0.4638,\n",
              "  0.4625,\n",
              "  0.4673,\n",
              "  0.4644,\n",
              "  0.4696,\n",
              "  0.4738,\n",
              "  0.4687,\n",
              "  0.4826,\n",
              "  0.4779,\n",
              "  0.4772,\n",
              "  0.4823,\n",
              "  0.4979,\n",
              "  0.4881,\n",
              "  0.4973,\n",
              "  0.499,\n",
              "  0.4942,\n",
              "  0.4917,\n",
              "  0.529,\n",
              "  0.5289,\n",
              "  0.5335,\n",
              "  0.5353,\n",
              "  0.538,\n",
              "  0.5349,\n",
              "  0.5354,\n",
              "  0.54,\n",
              "  0.5362,\n",
              "  0.5419])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bcxeAr3UOq43",
        "outputId": "a451b9b5-789b-49d9-d97a-24a640e18a6a"
      },
      "source": [
        "# accuracy after 50 epochs\n",
        "all_losses, predicted_labels, true_labels = predict(model, val_dataloader, criterion, device)\n",
        "print(len(predicted_labels), len(val_dataset))\n",
        "assert len(predicted_labels) == len(val_dataset)\n",
        "accuracy = accuracy_score(predicted_labels, true_labels)\n",
        "print(accuracy)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "batch vloss: 2.0426, batch vacc: 0.5515: 100%|██████████| 20/20 [00:07<00:00,  2.83it/s]\n",
            "10000 10000\n",
            "0.5419\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XFsCfp6-O-bg"
      },
      "source": [
        "# change number of epochs and hope for the best (total is 50 + 20 = 70)\n",
        "n_epochs = 20\n",
        "# change lr (previous was 0.05 at start of the training, but reduced to 0.0005 due to scheduler)\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=0.0005, momentum=0.9, weight_decay=5e-4) # YOUR OPTIMIZER\n",
        "scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer, milestones=[6,13], gamma=0.1) # LR SCHEDULE THAT YOU PROBABLY CHOOSE"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RxxXDqRKPfhB",
        "outputId": "db74e3ab-a5fb-4e1f-8d29-f431a4865e6b"
      },
      "source": [
        "train(resnet18, train_dataloader, val_dataloader, criterion, optimizer, device, n_epochs, scheduler)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch[0] training...\n",
            "batch tloss: 2.2539, batch tacc: 0.4688: 100%|██████████| 782/782 [01:02<00:00, 12.42it/s]\n",
            "batch vloss: 2.0430, batch vacc: 0.5478: 100%|██████████| 20/20 [00:07<00:00,  2.80it/s]\n",
            "epoch loss: 2.1156, epoch acc: 0.5379\n",
            "Epoch[1] training...\n",
            "batch tloss: 2.3500, batch tacc: 0.4062: 100%|██████████| 782/782 [01:03<00:00, 12.32it/s]\n",
            "batch vloss: 2.0484, batch vacc: 0.5294: 100%|██████████| 20/20 [00:06<00:00,  2.87it/s]\n",
            "epoch loss: 2.0947, epoch acc: 0.5425\n",
            "Epoch[2] training...\n",
            "batch tloss: 2.7729, batch tacc: 0.4062: 100%|██████████| 782/782 [01:03<00:00, 12.40it/s]\n",
            "batch vloss: 2.0353, batch vacc: 0.5515: 100%|██████████| 20/20 [00:06<00:00,  2.86it/s]\n",
            "epoch loss: 2.0927, epoch acc: 0.5420\n",
            "Epoch[3] training...\n",
            "batch tloss: 2.3523, batch tacc: 0.5000: 100%|██████████| 782/782 [01:02<00:00, 12.43it/s]\n",
            "batch vloss: 2.0191, batch vacc: 0.5478: 100%|██████████| 20/20 [00:07<00:00,  2.84it/s]\n",
            "epoch loss: 2.1038, epoch acc: 0.5435\n",
            "Epoch[4] training...\n",
            "batch tloss: 2.7618, batch tacc: 0.3438: 100%|██████████| 782/782 [01:03<00:00, 12.39it/s]\n",
            "batch vloss: 2.0046, batch vacc: 0.5662: 100%|██████████| 20/20 [00:07<00:00,  2.84it/s]\n",
            "epoch loss: 2.1012, epoch acc: 0.5409\n",
            "Epoch[5] training...\n",
            "batch tloss: 2.3047, batch tacc: 0.5625: 100%|██████████| 782/782 [01:02<00:00, 12.42it/s]\n",
            "batch vloss: 2.0071, batch vacc: 0.5699: 100%|██████████| 20/20 [00:07<00:00,  2.83it/s]\n",
            "epoch loss: 2.0868, epoch acc: 0.5456\n",
            "Epoch[6] training...\n",
            "batch tloss: 2.7574, batch tacc: 0.2812: 100%|██████████| 782/782 [01:03<00:00, 12.41it/s]\n",
            "batch vloss: 2.0073, batch vacc: 0.5478: 100%|██████████| 20/20 [00:07<00:00,  2.80it/s]\n",
            "epoch loss: 2.0872, epoch acc: 0.5469\n",
            "Epoch[7] training...\n",
            "batch tloss: 1.9517, batch tacc: 0.5938: 100%|██████████| 782/782 [01:02<00:00, 12.43it/s]\n",
            "batch vloss: 2.0222, batch vacc: 0.5478: 100%|██████████| 20/20 [00:07<00:00,  2.80it/s]\n",
            "epoch loss: 2.0906, epoch acc: 0.5437\n",
            "Epoch[8] training...\n",
            "batch tloss: 2.2498, batch tacc: 0.5938: 100%|██████████| 782/782 [01:03<00:00, 12.40it/s]\n",
            "batch vloss: 2.0343, batch vacc: 0.5478: 100%|██████████| 20/20 [00:07<00:00,  2.81it/s]\n",
            "epoch loss: 2.0833, epoch acc: 0.5456\n",
            "Epoch[9] training...\n",
            "batch tloss: 2.1584, batch tacc: 0.4375: 100%|██████████| 782/782 [01:03<00:00, 12.37it/s]\n",
            "batch vloss: 2.0172, batch vacc: 0.5515: 100%|██████████| 20/20 [00:07<00:00,  2.84it/s]\n",
            "epoch loss: 2.0911, epoch acc: 0.5449\n",
            "Epoch[10] training...\n",
            "batch tloss: 2.6549, batch tacc: 0.3750: 100%|██████████| 782/782 [01:03<00:00, 12.40it/s]\n",
            "batch vloss: 2.0269, batch vacc: 0.5478: 100%|██████████| 20/20 [00:07<00:00,  2.84it/s]\n",
            "epoch loss: 2.0788, epoch acc: 0.5464\n",
            "Epoch[11] training...\n",
            "batch tloss: 2.0879, batch tacc: 0.5625: 100%|██████████| 782/782 [01:03<00:00, 12.40it/s]\n",
            "batch vloss: 1.9808, batch vacc: 0.5699: 100%|██████████| 20/20 [00:07<00:00,  2.82it/s]\n",
            "epoch loss: 2.0717, epoch acc: 0.5475\n",
            "Epoch[12] training...\n",
            "batch tloss: 2.1530, batch tacc: 0.5312: 100%|██████████| 782/782 [01:03<00:00, 12.38it/s]\n",
            "batch vloss: 2.0251, batch vacc: 0.5478: 100%|██████████| 20/20 [00:07<00:00,  2.83it/s]\n",
            "epoch loss: 2.0766, epoch acc: 0.5480\n",
            "Epoch[13] training...\n",
            "batch tloss: 2.4468, batch tacc: 0.4688: 100%|██████████| 782/782 [01:03<00:00, 12.38it/s]\n",
            "batch vloss: 1.9843, batch vacc: 0.5625: 100%|██████████| 20/20 [00:07<00:00,  2.82it/s]\n",
            "epoch loss: 2.0780, epoch acc: 0.5443\n",
            "Epoch[14] training...\n",
            "batch tloss: 2.4988, batch tacc: 0.3125: 100%|██████████| 782/782 [01:03<00:00, 12.34it/s]\n",
            "batch vloss: 2.0017, batch vacc: 0.5662: 100%|██████████| 20/20 [00:07<00:00,  2.81it/s]\n",
            "epoch loss: 2.0798, epoch acc: 0.5475\n",
            "Epoch[15] training...\n",
            "batch tloss: 2.7183, batch tacc: 0.3750: 100%|██████████| 782/782 [01:03<00:00, 12.34it/s]\n",
            "batch vloss: 2.0002, batch vacc: 0.5478: 100%|██████████| 20/20 [00:06<00:00,  2.87it/s]\n",
            "epoch loss: 2.0647, epoch acc: 0.5466\n",
            "Epoch[16] training...\n",
            "batch tloss: 2.2479, batch tacc: 0.4688: 100%|██████████| 782/782 [01:03<00:00, 12.40it/s]\n",
            "batch vloss: 2.0034, batch vacc: 0.5478: 100%|██████████| 20/20 [00:07<00:00,  2.83it/s]\n",
            "epoch loss: 2.0767, epoch acc: 0.5490\n",
            "Epoch[17] training...\n",
            "batch tloss: 2.6635, batch tacc: 0.4375: 100%|██████████| 782/782 [01:03<00:00, 12.37it/s]\n",
            "batch vloss: 1.9828, batch vacc: 0.5515: 100%|██████████| 20/20 [00:07<00:00,  2.81it/s]\n",
            "epoch loss: 2.0808, epoch acc: 0.5450\n",
            "Epoch[18] training...\n",
            "batch tloss: 2.2862, batch tacc: 0.3750: 100%|██████████| 782/782 [01:03<00:00, 12.34it/s]\n",
            "batch vloss: 1.9968, batch vacc: 0.5515: 100%|██████████| 20/20 [00:07<00:00,  2.82it/s]\n",
            "epoch loss: 2.0708, epoch acc: 0.5469\n",
            "Epoch[19] training...\n",
            "batch tloss: 2.4404, batch tacc: 0.4375: 100%|██████████| 782/782 [01:02<00:00, 12.42it/s]\n",
            "batch vloss: 1.9636, batch vacc: 0.5735: 100%|██████████| 20/20 [00:07<00:00,  2.81it/s]\n",
            "epoch loss: 2.0754, epoch acc: 0.5467\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "([2.115617793798447,\n",
              "  2.094724100828171,\n",
              "  2.092661589384079,\n",
              "  2.1037700355052946,\n",
              "  2.1012390494346618,\n",
              "  2.086758667230606,\n",
              "  2.0872295260429383,\n",
              "  2.0906174004077913,\n",
              "  2.08331583738327,\n",
              "  2.091127133369446,\n",
              "  2.078837013244629,\n",
              "  2.071694868803024,\n",
              "  2.076632869243622,\n",
              "  2.0779599964618685,\n",
              "  2.0797977685928344,\n",
              "  2.064708250761032,\n",
              "  2.0766901910305022,\n",
              "  2.0807956755161285,\n",
              "  2.070765030384064,\n",
              "  2.0753883838653566],\n",
              " [0.5379,\n",
              "  0.5425,\n",
              "  0.542,\n",
              "  0.5435,\n",
              "  0.5409,\n",
              "  0.5456,\n",
              "  0.5469,\n",
              "  0.5437,\n",
              "  0.5456,\n",
              "  0.5449,\n",
              "  0.5464,\n",
              "  0.5475,\n",
              "  0.548,\n",
              "  0.5443,\n",
              "  0.5475,\n",
              "  0.5466,\n",
              "  0.549,\n",
              "  0.545,\n",
              "  0.5469,\n",
              "  0.5467])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tFedEk__PznA",
        "outputId": "e8869b58-37e8-45ab-f80e-900629a30fec"
      },
      "source": [
        "# accuracy after 70 epochs\n",
        "all_losses, predicted_labels, true_labels = predict(model, val_dataloader, criterion, device)\n",
        "print(len(predicted_labels), len(val_dataset))\n",
        "assert len(predicted_labels) == len(val_dataset)\n",
        "accuracy = accuracy_score(predicted_labels, true_labels)\n",
        "print(accuracy)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "batch vloss: 1.9636, batch vacc: 0.5735: 100%|██████████| 20/20 [00:07<00:00,  2.83it/s]\n",
            "10000 10000\n",
            "0.5467\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ETHqJbcxyS-m"
      },
      "source": [
        "# try to increase lr a bit and decrease again\n",
        "# change number of epochs and hope for the best (total is 70 + 20 = 90)\n",
        "n_epochs = 20\n",
        "# change lr (previous was 0.000005 after all schedulers worked)\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=0.0008, momentum=0.9, weight_decay=5e-4) # YOUR OPTIMIZER\n",
        "scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer, milestones=[6,13], gamma=0.1) # LR SCHEDULE THAT YOU PROBABLY CHOOSE"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LXPKU9SW4_7j",
        "outputId": "a8b8fa50-aec3-4a96-8655-96ef2ad02b41"
      },
      "source": [
        "train(resnet18, train_dataloader, val_dataloader, criterion, optimizer, device, n_epochs, scheduler)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch[0] training...\n",
            "batch tloss: 2.6029, batch tacc: 0.3125: 100%|██████████| 782/782 [01:03<00:00, 12.29it/s]\n",
            "batch vloss: 2.0385, batch vacc: 0.5368: 100%|██████████| 20/20 [00:07<00:00,  2.83it/s]\n",
            "epoch loss: 2.0831, epoch acc: 0.5442\n",
            "Epoch[1] training...\n",
            "batch tloss: 2.4110, batch tacc: 0.4062: 100%|██████████| 782/782 [01:03<00:00, 12.34it/s]\n",
            "batch vloss: 2.0608, batch vacc: 0.5478: 100%|██████████| 20/20 [00:07<00:00,  2.82it/s]\n",
            "epoch loss: 2.0605, epoch acc: 0.5434\n",
            "Epoch[2] training...\n",
            "batch tloss: 2.3520, batch tacc: 0.5312: 100%|██████████| 782/782 [01:03<00:00, 12.32it/s]\n",
            "batch vloss: 1.9597, batch vacc: 0.5515: 100%|██████████| 20/20 [00:07<00:00,  2.78it/s]\n",
            "epoch loss: 2.0710, epoch acc: 0.5431\n",
            "Epoch[3] training...\n",
            "batch tloss: 3.3048, batch tacc: 0.2812: 100%|██████████| 782/782 [01:03<00:00, 12.34it/s]\n",
            "batch vloss: 1.9974, batch vacc: 0.5515: 100%|██████████| 20/20 [00:07<00:00,  2.77it/s]\n",
            "epoch loss: 2.0767, epoch acc: 0.5361\n",
            "Epoch[4] training...\n",
            "batch tloss: 2.5300, batch tacc: 0.4062: 100%|██████████| 782/782 [01:03<00:00, 12.35it/s]\n",
            "batch vloss: 2.0508, batch vacc: 0.5404: 100%|██████████| 20/20 [00:07<00:00,  2.76it/s]\n",
            "epoch loss: 2.0742, epoch acc: 0.5374\n",
            "Epoch[5] training...\n",
            "batch tloss: 3.0621, batch tacc: 0.3125: 100%|██████████| 782/782 [01:03<00:00, 12.37it/s]\n",
            "batch vloss: 2.0342, batch vacc: 0.5515: 100%|██████████| 20/20 [00:07<00:00,  2.82it/s]\n",
            "epoch loss: 2.0696, epoch acc: 0.5449\n",
            "Epoch[6] training...\n",
            "batch tloss: 2.2913, batch tacc: 0.3750: 100%|██████████| 782/782 [01:03<00:00, 12.37it/s]\n",
            "batch vloss: 2.0072, batch vacc: 0.5368: 100%|██████████| 20/20 [00:07<00:00,  2.78it/s]\n",
            "epoch loss: 2.0429, epoch acc: 0.5472\n",
            "Epoch[7] training...\n",
            "batch tloss: 2.2574, batch tacc: 0.5625: 100%|██████████| 782/782 [01:03<00:00, 12.31it/s]\n",
            "batch vloss: 1.9815, batch vacc: 0.5625: 100%|██████████| 20/20 [00:07<00:00,  2.78it/s]\n",
            "epoch loss: 2.0422, epoch acc: 0.5484\n",
            "Epoch[8] training...\n",
            "batch tloss: 2.1204, batch tacc: 0.5000: 100%|██████████| 782/782 [01:03<00:00, 12.35it/s]\n",
            "batch vloss: 1.9960, batch vacc: 0.5404: 100%|██████████| 20/20 [00:07<00:00,  2.82it/s]\n",
            "epoch loss: 2.0503, epoch acc: 0.5477\n",
            "Epoch[9] training...\n",
            "batch tloss: 3.1047, batch tacc: 0.3125: 100%|██████████| 782/782 [01:03<00:00, 12.35it/s]\n",
            "batch vloss: 1.9695, batch vacc: 0.5662: 100%|██████████| 20/20 [00:07<00:00,  2.81it/s]\n",
            "epoch loss: 2.0412, epoch acc: 0.5517\n",
            "Epoch[10] training...\n",
            "batch tloss: 2.0780, batch tacc: 0.5625: 100%|██████████| 782/782 [01:03<00:00, 12.27it/s]\n",
            "batch vloss: 1.9754, batch vacc: 0.5515: 100%|██████████| 20/20 [00:07<00:00,  2.84it/s]\n",
            "epoch loss: 2.0443, epoch acc: 0.5499\n",
            "Epoch[11] training...\n",
            "batch tloss: 2.1658, batch tacc: 0.5938: 100%|██████████| 782/782 [01:03<00:00, 12.27it/s]\n",
            "batch vloss: 1.9474, batch vacc: 0.5662: 100%|██████████| 20/20 [00:07<00:00,  2.81it/s]\n",
            "epoch loss: 2.0338, epoch acc: 0.5539\n",
            "Epoch[12] training...\n",
            "batch tloss: 2.1749, batch tacc: 0.4062: 100%|██████████| 782/782 [01:03<00:00, 12.29it/s]\n",
            "batch vloss: 1.9878, batch vacc: 0.5478: 100%|██████████| 20/20 [00:07<00:00,  2.82it/s]\n",
            "epoch loss: 2.0510, epoch acc: 0.5504\n",
            "Epoch[13] training...\n",
            "batch tloss: 2.1590, batch tacc: 0.5000: 100%|██████████| 782/782 [01:03<00:00, 12.28it/s]\n",
            "batch vloss: 1.9923, batch vacc: 0.5551: 100%|██████████| 20/20 [00:07<00:00,  2.84it/s]\n",
            "epoch loss: 2.0430, epoch acc: 0.5503\n",
            "Epoch[14] training...\n",
            "batch tloss: 2.5083, batch tacc: 0.3750: 100%|██████████| 782/782 [01:03<00:00, 12.34it/s]\n",
            "batch vloss: 1.9774, batch vacc: 0.5515: 100%|██████████| 20/20 [00:07<00:00,  2.83it/s]\n",
            "epoch loss: 2.0411, epoch acc: 0.5500\n",
            "Epoch[15] training...\n",
            "batch tloss: 2.5056, batch tacc: 0.3438: 100%|██████████| 782/782 [01:03<00:00, 12.32it/s]\n",
            "batch vloss: 1.9951, batch vacc: 0.5441: 100%|██████████| 20/20 [00:07<00:00,  2.82it/s]\n",
            "epoch loss: 2.0265, epoch acc: 0.5526\n",
            "Epoch[16] training...\n",
            "batch tloss: 2.2135, batch tacc: 0.5312: 100%|██████████| 782/782 [01:03<00:00, 12.23it/s]\n",
            "batch vloss: 2.0020, batch vacc: 0.5551: 100%|██████████| 20/20 [00:07<00:00,  2.82it/s]\n",
            "epoch loss: 2.0274, epoch acc: 0.5508\n",
            "Epoch[17] training...\n",
            "batch tloss: 2.6251, batch tacc: 0.4062: 100%|██████████| 782/782 [01:03<00:00, 12.27it/s]\n",
            "batch vloss: 2.0045, batch vacc: 0.5551: 100%|██████████| 20/20 [00:07<00:00,  2.83it/s]\n",
            "epoch loss: 2.0485, epoch acc: 0.5499\n",
            "Epoch[18] training...\n",
            "batch tloss: 2.1316, batch tacc: 0.5000: 100%|██████████| 782/782 [01:03<00:00, 12.32it/s]\n",
            "batch vloss: 1.9793, batch vacc: 0.5478: 100%|██████████| 20/20 [00:07<00:00,  2.82it/s]\n",
            "epoch loss: 2.0483, epoch acc: 0.5492\n",
            "Epoch[19] training...\n",
            "batch tloss: 2.7641, batch tacc: 0.3750: 100%|██████████| 782/782 [01:03<00:00, 12.31it/s]\n",
            "batch vloss: 2.0144, batch vacc: 0.5478: 100%|██████████| 20/20 [00:07<00:00,  2.78it/s]\n",
            "epoch loss: 2.0350, epoch acc: 0.5551\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "([2.0831023156642914,\n",
              "  2.060468065738678,\n",
              "  2.071043002605438,\n",
              "  2.0767474114894866,\n",
              "  2.0742180585861205,\n",
              "  2.0695657253265383,\n",
              "  2.0428754687309265,\n",
              "  2.0421570420265196,\n",
              "  2.0502575814723967,\n",
              "  2.0411776065826417,\n",
              "  2.044291985034943,\n",
              "  2.0338232576847077,\n",
              "  2.051021361351013,\n",
              "  2.043023157119751,\n",
              "  2.041088891029358,\n",
              "  2.0264786541461945,\n",
              "  2.0273510575294496,\n",
              "  2.0485211610794067,\n",
              "  2.048312747478485,\n",
              "  2.0349574625492095],\n",
              " [0.5442,\n",
              "  0.5434,\n",
              "  0.5431,\n",
              "  0.5361,\n",
              "  0.5374,\n",
              "  0.5449,\n",
              "  0.5472,\n",
              "  0.5484,\n",
              "  0.5477,\n",
              "  0.5517,\n",
              "  0.5499,\n",
              "  0.5539,\n",
              "  0.5504,\n",
              "  0.5503,\n",
              "  0.55,\n",
              "  0.5526,\n",
              "  0.5508,\n",
              "  0.5499,\n",
              "  0.5492,\n",
              "  0.5551])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TI3i8NTpE9bn"
      },
      "source": [
        "# try hard once again (increase and decrease once)\n",
        "# change number of epochs and hope for the best (total is 90 + 20 = 110)\n",
        "n_epochs = 20\n",
        "# change lr (previous was 0.000008)\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=0.00008, momentum=0.9, weight_decay=5e-4) # YOUR OPTIMIZE\n",
        "scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer, milestones=[15], gamma=0.1) # LR SCHEDULE THAT YOU PROBABLY CHOOSE"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nfX9rGjuFGZW",
        "outputId": "c45a5715-dfc4-4600-8a01-257ac472119f"
      },
      "source": [
        "train(resnet18, train_dataloader, val_dataloader, criterion, optimizer, device, n_epochs, scheduler)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch[0] training...\n",
            "batch tloss: 1.9832, batch tacc: 0.5312: 100%|██████████| 782/782 [01:03<00:00, 12.33it/s]\n",
            "batch vloss: 1.9858, batch vacc: 0.5404: 100%|██████████| 20/20 [00:07<00:00,  2.80it/s]\n",
            "epoch loss: 2.0383, epoch acc: 0.5531\n",
            "Epoch[1] training...\n",
            "batch tloss: 2.4498, batch tacc: 0.4062: 100%|██████████| 782/782 [01:03<00:00, 12.38it/s]\n",
            "batch vloss: 1.9726, batch vacc: 0.5625: 100%|██████████| 20/20 [00:06<00:00,  2.89it/s]\n",
            "epoch loss: 2.0426, epoch acc: 0.5506\n",
            "Epoch[2] training...\n",
            "batch tloss: 2.3001, batch tacc: 0.4062: 100%|██████████| 782/782 [01:03<00:00, 12.36it/s]\n",
            "batch vloss: 1.9972, batch vacc: 0.5441: 100%|██████████| 20/20 [00:07<00:00,  2.79it/s]\n",
            "epoch loss: 2.0316, epoch acc: 0.5535\n",
            "Epoch[3] training...\n",
            "batch tloss: 2.3498, batch tacc: 0.4062: 100%|██████████| 782/782 [01:03<00:00, 12.28it/s]\n",
            "batch vloss: 1.9880, batch vacc: 0.5478: 100%|██████████| 20/20 [00:07<00:00,  2.82it/s]\n",
            "epoch loss: 2.0325, epoch acc: 0.5510\n",
            "Epoch[4] training...\n",
            "batch tloss: 2.1486, batch tacc: 0.5312: 100%|██████████| 782/782 [01:03<00:00, 12.34it/s]\n",
            "batch vloss: 2.0027, batch vacc: 0.5515: 100%|██████████| 20/20 [00:07<00:00,  2.82it/s]\n",
            "epoch loss: 2.0374, epoch acc: 0.5527\n",
            "Epoch[5] training...\n",
            "batch tloss: 2.4239, batch tacc: 0.4375: 100%|██████████| 782/782 [01:03<00:00, 12.38it/s]\n",
            "batch vloss: 1.9990, batch vacc: 0.5441: 100%|██████████| 20/20 [00:07<00:00,  2.81it/s]\n",
            "epoch loss: 2.0299, epoch acc: 0.5510\n",
            "Epoch[6] training...\n",
            "batch tloss: 2.7287, batch tacc: 0.3125: 100%|██████████| 782/782 [01:03<00:00, 12.32it/s]\n",
            "batch vloss: 1.9720, batch vacc: 0.5515: 100%|██████████| 20/20 [00:07<00:00,  2.86it/s]\n",
            "epoch loss: 2.0288, epoch acc: 0.5500\n",
            "Epoch[7] training...\n",
            "batch tloss: 2.5701, batch tacc: 0.4375: 100%|██████████| 782/782 [01:03<00:00, 12.32it/s]\n",
            "batch vloss: 1.9864, batch vacc: 0.5441: 100%|██████████| 20/20 [00:07<00:00,  2.84it/s]\n",
            "epoch loss: 2.0346, epoch acc: 0.5504\n",
            "Epoch[8] training...\n",
            "batch tloss: 2.6778, batch tacc: 0.4062: 100%|██████████| 782/782 [01:03<00:00, 12.38it/s]\n",
            "batch vloss: 1.9894, batch vacc: 0.5551: 100%|██████████| 20/20 [00:07<00:00,  2.84it/s]\n",
            "epoch loss: 2.0287, epoch acc: 0.5564\n",
            "Epoch[9] training...\n",
            "batch tloss: 2.6619, batch tacc: 0.3438: 100%|██████████| 782/782 [01:03<00:00, 12.31it/s]\n",
            "batch vloss: 1.9911, batch vacc: 0.5478: 100%|██████████| 20/20 [00:07<00:00,  2.81it/s]\n",
            "epoch loss: 2.0381, epoch acc: 0.5515\n",
            "Epoch[10] training...\n",
            "batch tloss: 3.2002, batch tacc: 0.2812: 100%|██████████| 782/782 [01:03<00:00, 12.28it/s]\n",
            "batch vloss: 2.0014, batch vacc: 0.5441: 100%|██████████| 20/20 [00:07<00:00,  2.77it/s]\n",
            "epoch loss: 2.0356, epoch acc: 0.5535\n",
            "Epoch[11] training...\n",
            "batch tloss: 2.6823, batch tacc: 0.3125: 100%|██████████| 782/782 [01:02<00:00, 12.46it/s]\n",
            "batch vloss: 1.9387, batch vacc: 0.5662: 100%|██████████| 20/20 [00:06<00:00,  2.88it/s]\n",
            "epoch loss: 2.0363, epoch acc: 0.5503\n",
            "Epoch[12] training...\n",
            "batch tloss: 2.9569, batch tacc: 0.3125: 100%|██████████| 782/782 [01:03<00:00, 12.39it/s]\n",
            "batch vloss: 1.9706, batch vacc: 0.5662: 100%|██████████| 20/20 [00:07<00:00,  2.81it/s]\n",
            "epoch loss: 2.0285, epoch acc: 0.5508\n",
            "Epoch[13] training...\n",
            "batch tloss: 2.1454, batch tacc: 0.4688: 100%|██████████| 782/782 [01:03<00:00, 12.30it/s]\n",
            "batch vloss: 1.9772, batch vacc: 0.5515: 100%|██████████| 20/20 [00:07<00:00,  2.84it/s]\n",
            "epoch loss: 2.0330, epoch acc: 0.5511\n",
            "Epoch[14] training...\n",
            "batch tloss: 2.5508, batch tacc: 0.4375: 100%|██████████| 782/782 [01:03<00:00, 12.29it/s]\n",
            "batch vloss: 1.9870, batch vacc: 0.5478: 100%|██████████| 20/20 [00:07<00:00,  2.80it/s]\n",
            "epoch loss: 2.0432, epoch acc: 0.5486\n",
            "Epoch[15] training...\n",
            "batch tloss: 2.6490, batch tacc: 0.4062: 100%|██████████| 782/782 [01:04<00:00, 12.20it/s]\n",
            "batch vloss: 2.0271, batch vacc: 0.5441: 100%|██████████| 20/20 [00:07<00:00,  2.77it/s]\n",
            "epoch loss: 2.0272, epoch acc: 0.5544\n",
            "Epoch[16] training...\n",
            "batch tloss: 2.6581, batch tacc: 0.3125: 100%|██████████| 782/782 [01:03<00:00, 12.25it/s]\n",
            "batch vloss: 1.9858, batch vacc: 0.5551: 100%|██████████| 20/20 [00:07<00:00,  2.80it/s]\n",
            "epoch loss: 2.0315, epoch acc: 0.5521\n",
            "Epoch[17] training...\n",
            "batch tloss: 2.3833, batch tacc: 0.3438: 100%|██████████| 782/782 [01:03<00:00, 12.26it/s]\n",
            "batch vloss: 1.9800, batch vacc: 0.5478: 100%|██████████| 20/20 [00:07<00:00,  2.83it/s]\n",
            "epoch loss: 2.0299, epoch acc: 0.5499\n",
            "Epoch[18] training...\n",
            "batch tloss: 2.5857, batch tacc: 0.4062: 100%|██████████| 782/782 [01:03<00:00, 12.33it/s]\n",
            "batch vloss: 1.9967, batch vacc: 0.5441: 100%|██████████| 20/20 [00:07<00:00,  2.81it/s]\n",
            "epoch loss: 2.0202, epoch acc: 0.5545\n",
            "Epoch[19] training...\n",
            "batch tloss: 2.3899, batch tacc: 0.5000: 100%|██████████| 782/782 [01:03<00:00, 12.24it/s]\n",
            "batch vloss: 1.9577, batch vacc: 0.5515: 100%|██████████| 20/20 [00:07<00:00,  2.78it/s]\n",
            "epoch loss: 2.0248, epoch acc: 0.5528\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "([2.038334387540817,\n",
              "  2.042556118965149,\n",
              "  2.0315613031387327,\n",
              "  2.0325495421886446,\n",
              "  2.037391424179077,\n",
              "  2.0299007892608643,\n",
              "  2.028788465261459,\n",
              "  2.034645140171051,\n",
              "  2.028744751214981,\n",
              "  2.0380656480789185,\n",
              "  2.035568428039551,\n",
              "  2.036299777030945,\n",
              "  2.028529042005539,\n",
              "  2.0330076456069945,\n",
              "  2.043207365274429,\n",
              "  2.0271693766117096,\n",
              "  2.03148358464241,\n",
              "  2.029901909828186,\n",
              "  2.0202405869960787,\n",
              "  2.0247882902622223],\n",
              " [0.5531,\n",
              "  0.5506,\n",
              "  0.5535,\n",
              "  0.551,\n",
              "  0.5527,\n",
              "  0.551,\n",
              "  0.55,\n",
              "  0.5504,\n",
              "  0.5564,\n",
              "  0.5515,\n",
              "  0.5535,\n",
              "  0.5503,\n",
              "  0.5508,\n",
              "  0.5511,\n",
              "  0.5486,\n",
              "  0.5544,\n",
              "  0.5521,\n",
              "  0.5499,\n",
              "  0.5545,\n",
              "  0.5528])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p-lswNnFJMQK"
      },
      "source": [
        "# try hard once again (increase and decrease once)\n",
        "# change number of epochs and hope for the best (total is 110 + 50 = 160)\n",
        "n_epochs = 50\n",
        "# change lr (previous was 0.000008)\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=0.008, momentum=0.9, weight_decay=5e-4) # YOUR OPTIMIZE\n",
        "scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer, milestones=[20, 40], gamma=0.1) # LR SCHEDULE THAT YOU PROBABLY CHOOSE"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZcF-qW6CJuTU",
        "outputId": "ef0e1ab1-d270-4d9d-d071-d09c26c1e637"
      },
      "source": [
        "train(resnet18, train_dataloader, val_dataloader, criterion, optimizer, device, n_epochs, scheduler)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch[0] training...\n",
            "batch tloss: 2.7313, batch tacc: 0.2500: 100%|██████████| 782/782 [01:03<00:00, 12.36it/s]\n",
            "batch vloss: 2.3232, batch vacc: 0.4485: 100%|██████████| 20/20 [00:07<00:00,  2.79it/s]\n",
            "epoch loss: 2.4885, epoch acc: 0.4371\n",
            "Epoch[1] training...\n",
            "batch tloss: 3.3938, batch tacc: 0.2188: 100%|██████████| 782/782 [01:03<00:00, 12.33it/s]\n",
            "batch vloss: 2.2122, batch vacc: 0.5037: 100%|██████████| 20/20 [00:07<00:00,  2.81it/s]\n",
            "epoch loss: 2.4354, epoch acc: 0.4528\n",
            "Epoch[2] training...\n",
            "batch tloss: 3.4193, batch tacc: 0.2188: 100%|██████████| 782/782 [01:03<00:00, 12.34it/s]\n",
            "batch vloss: 2.3789, batch vacc: 0.4559: 100%|██████████| 20/20 [00:06<00:00,  2.86it/s]\n",
            "epoch loss: 2.3519, epoch acc: 0.4754\n",
            "Epoch[3] training...\n",
            "batch tloss: 2.6743, batch tacc: 0.4062: 100%|██████████| 782/782 [01:03<00:00, 12.39it/s]\n",
            "batch vloss: 2.4132, batch vacc: 0.4191: 100%|██████████| 20/20 [00:07<00:00,  2.81it/s]\n",
            "epoch loss: 2.3766, epoch acc: 0.4656\n",
            "Epoch[4] training...\n",
            "batch tloss: 2.8033, batch tacc: 0.3125: 100%|██████████| 782/782 [01:03<00:00, 12.40it/s]\n",
            "batch vloss: 2.1662, batch vacc: 0.5184: 100%|██████████| 20/20 [00:07<00:00,  2.76it/s]\n",
            "epoch loss: 2.3481, epoch acc: 0.4754\n",
            "Epoch[5] training...\n",
            "batch tloss: 2.9211, batch tacc: 0.3125: 100%|██████████| 782/782 [01:02<00:00, 12.42it/s]\n",
            "batch vloss: 2.4827, batch vacc: 0.4154: 100%|██████████| 20/20 [00:07<00:00,  2.78it/s]\n",
            "epoch loss: 2.3302, epoch acc: 0.4828\n",
            "Epoch[6] training...\n",
            "batch tloss: 3.0023, batch tacc: 0.4062: 100%|██████████| 782/782 [01:03<00:00, 12.37it/s]\n",
            "batch vloss: 2.1878, batch vacc: 0.5221: 100%|██████████| 20/20 [00:07<00:00,  2.81it/s]\n",
            "epoch loss: 2.3718, epoch acc: 0.4737\n",
            "Epoch[7] training...\n",
            "batch tloss: 2.7402, batch tacc: 0.4375: 100%|██████████| 782/782 [01:03<00:00, 12.37it/s]\n",
            "batch vloss: 2.2397, batch vacc: 0.4338: 100%|██████████| 20/20 [00:07<00:00,  2.82it/s]\n",
            "epoch loss: 2.3581, epoch acc: 0.4746\n",
            "Epoch[8] training...\n",
            "batch tloss: 2.9691, batch tacc: 0.4062: 100%|██████████| 782/782 [01:03<00:00, 12.34it/s]\n",
            "batch vloss: 2.1780, batch vacc: 0.5184: 100%|██████████| 20/20 [00:07<00:00,  2.80it/s]\n",
            "epoch loss: 2.3338, epoch acc: 0.4807\n",
            "Epoch[9] training...\n",
            "batch tloss: 2.3071, batch tacc: 0.4375: 100%|██████████| 782/782 [01:03<00:00, 12.37it/s]\n",
            "batch vloss: 2.1045, batch vacc: 0.5294: 100%|██████████| 20/20 [00:07<00:00,  2.83it/s]\n",
            "epoch loss: 2.3494, epoch acc: 0.4815\n",
            "Epoch[10] training...\n",
            "batch tloss: 2.9191, batch tacc: 0.3438: 100%|██████████| 782/782 [01:03<00:00, 12.30it/s]\n",
            "batch vloss: 2.2405, batch vacc: 0.5294: 100%|██████████| 20/20 [00:07<00:00,  2.81it/s]\n",
            "epoch loss: 2.3376, epoch acc: 0.4770\n",
            "Epoch[11] training...\n",
            "batch tloss: 2.7145, batch tacc: 0.3750: 100%|██████████| 782/782 [01:03<00:00, 12.35it/s]\n",
            "batch vloss: 2.2059, batch vacc: 0.4963: 100%|██████████| 20/20 [00:06<00:00,  2.86it/s]\n",
            "epoch loss: 2.2726, epoch acc: 0.4976\n",
            "Epoch[12] training...\n",
            "batch tloss: 3.0664, batch tacc: 0.2500: 100%|██████████| 782/782 [01:03<00:00, 12.34it/s]\n",
            "batch vloss: 2.2085, batch vacc: 0.5368: 100%|██████████| 20/20 [00:06<00:00,  2.91it/s]\n",
            "epoch loss: 2.2973, epoch acc: 0.4914\n",
            "Epoch[13] training...\n",
            "batch tloss: 2.7529, batch tacc: 0.4062: 100%|██████████| 782/782 [01:03<00:00, 12.34it/s]\n",
            "batch vloss: 2.2625, batch vacc: 0.4816: 100%|██████████| 20/20 [00:07<00:00,  2.79it/s]\n",
            "epoch loss: 2.2705, epoch acc: 0.4928\n",
            "Epoch[14] training...\n",
            "batch tloss: 3.3439, batch tacc: 0.1875: 100%|██████████| 782/782 [01:03<00:00, 12.32it/s]\n",
            "batch vloss: 2.2655, batch vacc: 0.4779: 100%|██████████| 20/20 [00:07<00:00,  2.82it/s]\n",
            "epoch loss: 2.2858, epoch acc: 0.4820\n",
            "Epoch[15] training...\n",
            "batch tloss: 2.3852, batch tacc: 0.4062: 100%|██████████| 782/782 [01:03<00:00, 12.31it/s]\n",
            "batch vloss: 2.1636, batch vacc: 0.5147: 100%|██████████| 20/20 [00:07<00:00,  2.78it/s]\n",
            "epoch loss: 2.2731, epoch acc: 0.4967\n",
            "Epoch[16] training...\n",
            "batch tloss: 2.6990, batch tacc: 0.2500: 100%|██████████| 782/782 [01:03<00:00, 12.33it/s]\n",
            "batch vloss: 2.0537, batch vacc: 0.5515: 100%|██████████| 20/20 [00:07<00:00,  2.79it/s]\n",
            "epoch loss: 2.2918, epoch acc: 0.4864\n",
            "Epoch[17] training...\n",
            "batch tloss: 2.8673, batch tacc: 0.3125: 100%|██████████| 782/782 [01:03<00:00, 12.34it/s]\n",
            "batch vloss: 2.2322, batch vacc: 0.4669: 100%|██████████| 20/20 [00:07<00:00,  2.79it/s]\n",
            "epoch loss: 2.2098, epoch acc: 0.5050\n",
            "Epoch[18] training...\n",
            "batch tloss: 3.4081, batch tacc: 0.3125: 100%|██████████| 782/782 [01:03<00:00, 12.31it/s]\n",
            "batch vloss: 2.3381, batch vacc: 0.4412: 100%|██████████| 20/20 [00:07<00:00,  2.83it/s]\n",
            "epoch loss: 2.2371, epoch acc: 0.4938\n",
            "Epoch[19] training...\n",
            "batch tloss: 2.8173, batch tacc: 0.2500: 100%|██████████| 782/782 [01:03<00:00, 12.30it/s]\n",
            "batch vloss: 2.2776, batch vacc: 0.4926: 100%|██████████| 20/20 [00:07<00:00,  2.84it/s]\n",
            "epoch loss: 2.2811, epoch acc: 0.4830\n",
            "Epoch[20] training...\n",
            "batch tloss: 2.6523, batch tacc: 0.4062: 100%|██████████| 782/782 [01:03<00:00, 12.31it/s]\n",
            "batch vloss: 2.0057, batch vacc: 0.5551: 100%|██████████| 20/20 [00:07<00:00,  2.80it/s]\n",
            "epoch loss: 2.0164, epoch acc: 0.5604\n",
            "Epoch[21] training...\n",
            "batch tloss: 2.4949, batch tacc: 0.3750: 100%|██████████| 782/782 [01:03<00:00, 12.29it/s]\n",
            "batch vloss: 2.0068, batch vacc: 0.5478: 100%|██████████| 20/20 [00:07<00:00,  2.80it/s]\n",
            "epoch loss: 1.9905, epoch acc: 0.5636\n",
            "Epoch[22] training...\n",
            "batch tloss: 2.7497, batch tacc: 0.3750: 100%|██████████| 782/782 [01:03<00:00, 12.30it/s]\n",
            "batch vloss: 2.0162, batch vacc: 0.5662: 100%|██████████| 20/20 [00:07<00:00,  2.79it/s]\n",
            "epoch loss: 1.9709, epoch acc: 0.5652\n",
            "Epoch[23] training...\n",
            "batch tloss: 2.5608, batch tacc: 0.4062: 100%|██████████| 782/782 [01:03<00:00, 12.24it/s]\n",
            "batch vloss: 1.9245, batch vacc: 0.5699: 100%|██████████| 20/20 [00:07<00:00,  2.80it/s]\n",
            "epoch loss: 1.9541, epoch acc: 0.5697\n",
            "Epoch[24] training...\n",
            "batch tloss: 2.1887, batch tacc: 0.4375: 100%|██████████| 782/782 [01:03<00:00, 12.29it/s]\n",
            "batch vloss: 1.9455, batch vacc: 0.5551: 100%|██████████| 20/20 [00:07<00:00,  2.80it/s]\n",
            "epoch loss: 1.9484, epoch acc: 0.5695\n",
            "Epoch[25] training...\n",
            "batch tloss: 2.5318, batch tacc: 0.4375: 100%|██████████| 782/782 [01:03<00:00, 12.27it/s]\n",
            "batch vloss: 1.9081, batch vacc: 0.5662: 100%|██████████| 20/20 [00:07<00:00,  2.84it/s]\n",
            "epoch loss: 1.9299, epoch acc: 0.5729\n",
            "Epoch[26] training...\n",
            "batch tloss: 2.1966, batch tacc: 0.4688: 100%|██████████| 782/782 [01:03<00:00, 12.27it/s]\n",
            "batch vloss: 1.9419, batch vacc: 0.5404: 100%|██████████| 20/20 [00:07<00:00,  2.84it/s]\n",
            "epoch loss: 1.9286, epoch acc: 0.5750\n",
            "Epoch[27] training...\n",
            "batch tloss: 2.2129, batch tacc: 0.5000: 100%|██████████| 782/782 [01:04<00:00, 12.21it/s]\n",
            "batch vloss: 1.8955, batch vacc: 0.5662: 100%|██████████| 20/20 [00:06<00:00,  2.86it/s]\n",
            "epoch loss: 1.9151, epoch acc: 0.5738\n",
            "Epoch[28] training...\n",
            "batch tloss: 2.3647, batch tacc: 0.5312: 100%|██████████| 782/782 [01:04<00:00, 12.22it/s]\n",
            "batch vloss: 1.9126, batch vacc: 0.5625: 100%|██████████| 20/20 [00:07<00:00,  2.82it/s]\n",
            "epoch loss: 1.9088, epoch acc: 0.5765\n",
            "Epoch[29] training...\n",
            "batch tloss: 2.5195, batch tacc: 0.4375: 100%|██████████| 782/782 [01:03<00:00, 12.27it/s]\n",
            "batch vloss: 1.9116, batch vacc: 0.5551: 100%|██████████| 20/20 [00:07<00:00,  2.82it/s]\n",
            "epoch loss: 1.8934, epoch acc: 0.5783\n",
            "Epoch[30] training...\n",
            "batch tloss: 2.5258, batch tacc: 0.3750: 100%|██████████| 782/782 [01:03<00:00, 12.24it/s]\n",
            "batch vloss: 1.8981, batch vacc: 0.5662: 100%|██████████| 20/20 [00:07<00:00,  2.84it/s]\n",
            "epoch loss: 1.8973, epoch acc: 0.5803\n",
            "Epoch[31] training...\n",
            "batch tloss: 1.9987, batch tacc: 0.5312: 100%|██████████| 782/782 [01:03<00:00, 12.26it/s]\n",
            "batch vloss: 1.9078, batch vacc: 0.5588: 100%|██████████| 20/20 [00:07<00:00,  2.80it/s]\n",
            "epoch loss: 1.8878, epoch acc: 0.5777\n",
            "Epoch[32] training...\n",
            "batch tloss: 2.5641, batch tacc: 0.3438: 100%|██████████| 782/782 [01:03<00:00, 12.26it/s]\n",
            "batch vloss: 1.8667, batch vacc: 0.5735: 100%|██████████| 20/20 [00:07<00:00,  2.82it/s]\n",
            "epoch loss: 1.8931, epoch acc: 0.5778\n",
            "Epoch[33] training...\n",
            "batch tloss: 2.0824, batch tacc: 0.5000: 100%|██████████| 782/782 [01:03<00:00, 12.31it/s]\n",
            "batch vloss: 1.8721, batch vacc: 0.5588: 100%|██████████| 20/20 [00:07<00:00,  2.82it/s]\n",
            "epoch loss: 1.8871, epoch acc: 0.5779\n",
            "Epoch[34] training...\n",
            "batch tloss: 2.4118, batch tacc: 0.3438: 100%|██████████| 782/782 [01:03<00:00, 12.26it/s]\n",
            "batch vloss: 1.7905, batch vacc: 0.6029: 100%|██████████| 20/20 [00:07<00:00,  2.81it/s]\n",
            "epoch loss: 1.8833, epoch acc: 0.5778\n",
            "Epoch[35] training...\n",
            "batch tloss: 2.1840, batch tacc: 0.4062: 100%|██████████| 782/782 [01:03<00:00, 12.30it/s]\n",
            "batch vloss: 1.8774, batch vacc: 0.5662: 100%|██████████| 20/20 [00:07<00:00,  2.76it/s]\n",
            "epoch loss: 1.8770, epoch acc: 0.5813\n",
            "Epoch[36] training...\n",
            "batch tloss: 2.1077, batch tacc: 0.4062: 100%|██████████| 782/782 [01:03<00:00, 12.26it/s]\n",
            "batch vloss: 1.8319, batch vacc: 0.5919: 100%|██████████| 20/20 [00:07<00:00,  2.80it/s]\n",
            "epoch loss: 1.8771, epoch acc: 0.5799\n",
            "Epoch[37] training...\n",
            "batch tloss: 2.3800, batch tacc: 0.4688: 100%|██████████| 782/782 [01:03<00:00, 12.27it/s]\n",
            "batch vloss: 1.8241, batch vacc: 0.5919: 100%|██████████| 20/20 [00:07<00:00,  2.80it/s]\n",
            "epoch loss: 1.8787, epoch acc: 0.5781\n",
            "Epoch[38] training...\n",
            "batch tloss: 1.9368, batch tacc: 0.5625: 100%|██████████| 782/782 [01:04<00:00, 12.18it/s]\n",
            "batch vloss: 1.8352, batch vacc: 0.5882: 100%|██████████| 20/20 [00:07<00:00,  2.81it/s]\n",
            "epoch loss: 1.8631, epoch acc: 0.5835\n",
            "Epoch[39] training...\n",
            "batch tloss: 2.2049, batch tacc: 0.4688: 100%|██████████| 782/782 [01:03<00:00, 12.25it/s]\n",
            "batch vloss: 1.8673, batch vacc: 0.5515: 100%|██████████| 20/20 [00:07<00:00,  2.79it/s]\n",
            "epoch loss: 1.8703, epoch acc: 0.5760\n",
            "Epoch[40] training...\n",
            "batch tloss: 2.1749, batch tacc: 0.4062: 100%|██████████| 782/782 [01:03<00:00, 12.28it/s]\n",
            "batch vloss: 1.8418, batch vacc: 0.5735: 100%|██████████| 20/20 [00:07<00:00,  2.78it/s]\n",
            "epoch loss: 1.8521, epoch acc: 0.5849\n",
            "Epoch[41] training...\n",
            "batch tloss: 2.4875, batch tacc: 0.4375: 100%|██████████| 782/782 [01:04<00:00, 12.15it/s]\n",
            "batch vloss: 1.8790, batch vacc: 0.5588: 100%|██████████| 20/20 [00:06<00:00,  2.86it/s]\n",
            "epoch loss: 1.8485, epoch acc: 0.5863\n",
            "Epoch[42] training...\n",
            "batch tloss: 2.0494, batch tacc: 0.5625: 100%|██████████| 782/782 [01:04<00:00, 12.15it/s]\n",
            "batch vloss: 1.8671, batch vacc: 0.5625: 100%|██████████| 20/20 [00:07<00:00,  2.83it/s]\n",
            "epoch loss: 1.8459, epoch acc: 0.5868\n",
            "Epoch[43] training...\n",
            "batch tloss: 2.1429, batch tacc: 0.5312: 100%|██████████| 782/782 [01:04<00:00, 12.17it/s]\n",
            "batch vloss: 1.8407, batch vacc: 0.5625: 100%|██████████| 20/20 [00:07<00:00,  2.81it/s]\n",
            "epoch loss: 1.8468, epoch acc: 0.5865\n",
            "Epoch[44] training...\n",
            "batch tloss: 2.5608, batch tacc: 0.3750: 100%|██████████| 782/782 [01:04<00:00, 12.17it/s]\n",
            "batch vloss: 1.8774, batch vacc: 0.5662: 100%|██████████| 20/20 [00:07<00:00,  2.83it/s]\n",
            "epoch loss: 1.8387, epoch acc: 0.5857\n",
            "Epoch[45] training...\n",
            "batch tloss: 2.0966, batch tacc: 0.5938: 100%|██████████| 782/782 [01:03<00:00, 12.26it/s]\n",
            "batch vloss: 1.8426, batch vacc: 0.5551: 100%|██████████| 20/20 [00:07<00:00,  2.77it/s]\n",
            "epoch loss: 1.8371, epoch acc: 0.5868\n",
            "Epoch[46] training...\n",
            "batch tloss: 2.1450, batch tacc: 0.5312: 100%|██████████| 782/782 [01:03<00:00, 12.23it/s]\n",
            "batch vloss: 1.8669, batch vacc: 0.5625: 100%|██████████| 20/20 [00:07<00:00,  2.77it/s]\n",
            "epoch loss: 1.8479, epoch acc: 0.5868\n",
            "Epoch[47] training...\n",
            "batch tloss: 2.3731, batch tacc: 0.5312: 100%|██████████| 782/782 [01:03<00:00, 12.23it/s]\n",
            "batch vloss: 1.8600, batch vacc: 0.5625: 100%|██████████| 20/20 [00:07<00:00,  2.77it/s]\n",
            "epoch loss: 1.8386, epoch acc: 0.5867\n",
            "Epoch[48] training...\n",
            "batch tloss: 1.9454, batch tacc: 0.5312: 100%|██████████| 782/782 [01:04<00:00, 12.21it/s]\n",
            "batch vloss: 1.8479, batch vacc: 0.5846: 100%|██████████| 20/20 [00:07<00:00,  2.78it/s]\n",
            "epoch loss: 1.8393, epoch acc: 0.5873\n",
            "Epoch[49] training...\n",
            "batch tloss: 2.3836, batch tacc: 0.4688: 100%|██████████| 782/782 [01:04<00:00, 12.17it/s]\n",
            "batch vloss: 1.8594, batch vacc: 0.5625: 100%|██████████| 20/20 [00:07<00:00,  2.79it/s]\n",
            "epoch loss: 1.8362, epoch acc: 0.5862\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "([2.4884654879570007,\n",
              "  2.4354085326194763,\n",
              "  2.3518980979919433,\n",
              "  2.3765501022338866,\n",
              "  2.3481061935424803,\n",
              "  2.330153787136078,\n",
              "  2.3718225598335265,\n",
              "  2.35806941986084,\n",
              "  2.33378427028656,\n",
              "  2.349356460571289,\n",
              "  2.3376346826553345,\n",
              "  2.2725826859474183,\n",
              "  2.297279065847397,\n",
              "  2.2704860091209413,\n",
              "  2.285806667804718,\n",
              "  2.2731137573719025,\n",
              "  2.2917776226997377,\n",
              "  2.209756338596344,\n",
              "  2.2370642840862276,\n",
              "  2.2810950458049772,\n",
              "  2.0164288878440857,\n",
              "  1.9905426800251007,\n",
              "  1.9709449112415314,\n",
              "  1.9541387259960175,\n",
              "  1.9483651578426362,\n",
              "  1.929926908016205,\n",
              "  1.9285638272762298,\n",
              "  1.9151043713092804,\n",
              "  1.9087697327136994,\n",
              "  1.893426352739334,\n",
              "  1.8973443329334259,\n",
              "  1.8878269731998443,\n",
              "  1.8930960178375245,\n",
              "  1.8870680153369903,\n",
              "  1.8832656621932984,\n",
              "  1.8770473718643188,\n",
              "  1.8770743787288666,\n",
              "  1.8787112057209014,\n",
              "  1.8631231844425202,\n",
              "  1.8703285336494446,\n",
              "  1.8521351099014283,\n",
              "  1.8485256493091584,\n",
              "  1.8458992779254912,\n",
              "  1.846769279241562,\n",
              "  1.838661104440689,\n",
              "  1.8371204257011413,\n",
              "  1.847924566268921,\n",
              "  1.8385542273521422,\n",
              "  1.839277046918869,\n",
              "  1.836151158809662],\n",
              " [0.4371,\n",
              "  0.4528,\n",
              "  0.4754,\n",
              "  0.4656,\n",
              "  0.4754,\n",
              "  0.4828,\n",
              "  0.4737,\n",
              "  0.4746,\n",
              "  0.4807,\n",
              "  0.4815,\n",
              "  0.477,\n",
              "  0.4976,\n",
              "  0.4914,\n",
              "  0.4928,\n",
              "  0.482,\n",
              "  0.4967,\n",
              "  0.4864,\n",
              "  0.505,\n",
              "  0.4938,\n",
              "  0.483,\n",
              "  0.5604,\n",
              "  0.5636,\n",
              "  0.5652,\n",
              "  0.5697,\n",
              "  0.5695,\n",
              "  0.5729,\n",
              "  0.575,\n",
              "  0.5738,\n",
              "  0.5765,\n",
              "  0.5783,\n",
              "  0.5803,\n",
              "  0.5777,\n",
              "  0.5778,\n",
              "  0.5779,\n",
              "  0.5778,\n",
              "  0.5813,\n",
              "  0.5799,\n",
              "  0.5781,\n",
              "  0.5835,\n",
              "  0.576,\n",
              "  0.5849,\n",
              "  0.5863,\n",
              "  0.5868,\n",
              "  0.5865,\n",
              "  0.5857,\n",
              "  0.5868,\n",
              "  0.5868,\n",
              "  0.5867,\n",
              "  0.5873,\n",
              "  0.5862])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IUDl09WESaik",
        "outputId": "9136086b-9d8a-4c15-9c1f-4a3198dcf6f5"
      },
      "source": [
        "# accuracy after 160 epochs\n",
        "all_losses, predicted_labels, true_labels = predict(model, val_dataloader, criterion, device)\n",
        "print(len(predicted_labels), len(val_dataset))\n",
        "assert len(predicted_labels) == len(val_dataset)\n",
        "accuracy = accuracy_score(predicted_labels, true_labels)\n",
        "print(accuracy)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "batch vloss: 1.8594, batch vacc: 0.5625: 100%|██████████| 20/20 [00:07<00:00,  2.80it/s]\n",
            "10000 10000\n",
            "0.5862\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DkUgLHbxWRjP"
      },
      "source": [
        "Сохраним модель. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rLyOg7Y7UnoZ"
      },
      "source": [
        "PATH = \"/content/drive/MyDrive/resnet18-200.tar\"\n",
        "torch.save({\n",
        "            'epoch': 160,\n",
        "            'model_state_dict': model.state_dict(),\n",
        "            'optimizer_state_dict': optimizer.state_dict(),\n",
        "            }, PATH)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PQ62J7RVWxc-"
      },
      "source": [
        "Вычислим чексумму тар-файла. К решению будет приложен сам tar-file, чтобы была возможность проверить модель на факт изменений."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SbzkkHtSYIBh",
        "outputId": "4ecc4991-fff5-4a70-d598-247963cb23c3"
      },
      "source": [
        "! pwd"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l8UDT_y2V-43"
      },
      "source": [
        "! shasum -a 512 /content/drive/MyDrive/resnet18-200.tar > ./model_checksum.txt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C4kwH1stYnMV",
        "outputId": "fd61058e-7e58-4f5f-82de-7f8e9586a414"
      },
      "source": [
        "! cat model_checksum.txt"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "823b17566a73c391de49457ce287e071054df3a793666a7ce16130d7968b836d055b97ca000704ce2fb1de62d75b658854e784acd0348c8cc7796d93b58e0f49  /content/drive/MyDrive/resnet18-200.tar\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gQ_X7mHwZRiO"
      },
      "source": [
        "Достаточно будет скачать приложенный tar-file и запусить команду `shasum -a 512 path_to_file`, где path_to_file - путь к скачанному тар-файлу и сравнить чексумму, приложенную здесь с локальной версией."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aPNLgeemYJIF"
      },
      "source": [
        "__Ваш отчёт о проделанных экспериментах__:\n",
        "\n",
        "Будем пользоваться трюками, которые предлагаются для улучшения качества для Cifar. \n",
        "\n",
        "Аугментации:\n",
        "\n",
        "\n",
        "*   **Padding** (если картинка меньше 32х32, то дополнить картинку до необходимого размера). Если честно, есть ощущение, что на самом деле все картинки имеют размерность большую 32х32, поэтому на самом деле эта аугментация ничего не делает. Значение 4 в поле value на самом деле тоже является взятым наугад в непонимании того, как устроена функция PadIfNeeded из библиотеки Albumentations. В общем, ее скорее всего можно убрать.\n",
        "*   **Random Crop**. Уменьшаем размер картинки до 28х28. Я делал его в предположении, что в датасете находятся картинки в том числе и довольно маленького размера (меньшего чем 64х64). Чтобы мы либо захватывали почти все изображение, потому что для меньшего размера сетке будет сложно вычленить достаточное количество признаков. Обучение будет происходить быстрее, из-за меньших размеров.\n",
        "\n",
        "* **Flip**. Для разнообразия и от переобучения.\n",
        "\n",
        "* **Normalize**. Чтобы избежать \"взрыв градиента\" и для более быстрой оптимизации.\n",
        "\n",
        "*Honorable mention*:\n",
        "\n",
        "\n",
        "*   **CoarseDropout**. Вырезаем часть картинки, для предотвращения переобучения. Если честно, модель вместе с ним не добивалась необходимого результата, но была очень близка к нему (~0.54 accuracy). Для картинок большего размера, эффект был бы выше.\n",
        "\n",
        "Остальное многообразие аугментаций я не использовал, поскольку размер картинки довольно маленький. И всевозможные сдвиги, ротации и сдвиги цветовой гаммы мне не давали нужного результата. Возможно, я упустил какие-то интересные идеи, которые существуют конкретно для данного датасета без использования resize и буду рад знать о каких-либо других подходах и решениях проблемы.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xuFpMzP5NU1Z"
      },
      "source": [
        "Относительно реализации батчлоадера, используем более мелкий для трейна и более большой для валидации. Принципиального объяснения для этого нет. Хотелось чтобы эпоха занимала ~1 минуты, а валидация проходила быстрее. Похожих результатов получилось добиваться и с одинаковыми и с более мелкими батчами."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O2qSpu0bNfqM"
      },
      "source": [
        "Самое важное здесь это класс – ModifiedResNet18, который нужен нам для модификации ResNet модели. \n",
        "\n",
        "```\n",
        "class ModifiedResNet18(torch.nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.resnet = models.resnet18(pretrained=False)\n",
        "        self.resnet.conv1 = torch.nn.Conv2d(\n",
        "            3, 64, kernel_size=3, stride=1, padding=1, bias=False\n",
        "        )\n",
        "        self.resnet.maxpool = torch.nn.Identity()\n",
        "        self.resnet.fc = nn.Linear(512, 200)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.resnet(x)\n",
        "        return x\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ngXUGJZvOu1Y"
      },
      "source": [
        "Суть в том, что в архитекутре resnet18 из torchvision.models первый сверточный слой использует ядро размером 7х7 и является слишком большим для извлечения признаков, точнее, захватывает лишь общие признаки, а из-за низкой размерности, да еще и кропа, плюс stride=2 и padding=3, получается не очень удачный feature extraction. По факту первый сверточный слой в resnet18 заменяет несколько сверток 3х3 для ускорения обучения, что действительно имеет смысл на картинках большего размера После сверточного слоя, батч-нормализации и relu идет maxpool с ядром 3x3, уменьшая картинку до очень маленьких размеров. Опять же мы можем позволить себе большее количеством параметров, так как размеры картинок у нас маленькие. Поэтому maxpool мы можем просто убрать, так как downsampling в нашем случае является лишним.\n",
        "Некоторые из этих вещей описывают авторы оригинальной статьи о ResNet архитектуре (https://arxiv.org/pdf/1512.03385.pdf) при анализе архитектуры на датасете Cifar.\n",
        "Последний полносвязный слой меняем в соответствии с количеством классов в нашем датасете."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-f46oDCRW6xq"
      },
      "source": [
        "Теперь относительно обучения:\n",
        "Используем кросс-энтропию в качестве функции потерь для задачи классификации. Аналогичным образом можно использовать negative log likelihood + softmax.\n",
        "В качестве оптимизатора используем SGD с начальным lr=0.05 + scheduler, который каждые 20 эпох понижает lr.\n",
        "\n",
        "Такие параметры позволяют нам добиться качества в 0.54 за первые 50 эпох.\n",
        "А дальше начинается шаманство, которое не совсем мне понятно.\n",
        "\n",
        "Дальше мы продолжаем обучение и пытаемся на том же lr, после применения scheduler'a, уменьшить лосс. Возможно, что 15 эпох, это мало, для уменьшения lr, но у меня было четкое ощущение того, что на самом деле SGD скачет вокруг да около и продвинуться дальше не может. Так продолжается с 50 по 90 эпоху. Точнее он застревает в диапазоне 0.55 – 0.56.\n",
        "Я попытался несколько раз увеличить lr, потому что значения из разряда 5e-6 очень почти не двигают градиент. Но только единственное увеличение до 8e-3, сопровождающееся потерей качества и уменьшением lr через schduler позволили пройти порог в 0.56.\n",
        "\n",
        "Я руководствовался лишь тем, что lr нужно подкрутить в большую или в меньшую сторону для того, чтобы помочь ему спуститься, при этом слишком большой lr очевидно приводит к тому, что его начинает колбасить по сторонам, а слишком маленький – к тому что он почти не двигается.\n",
        "Возможно, можно за один заход придумать функцию для lr, которая бы позволила завершить обучение за один цикл, но у меня этого сделать не получилось."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "skzahk_Ja1Mn"
      },
      "source": [
        "Кроме статьи я руководствовался еще этим кодом, но он мне не особо помог:\n",
        "https://gist.github.com/y0ast/d91d09565462125a1eb75acc65da1469"
      ]
    }
  ]
}